import os
import random
from typing import List, Dict, Any, Tuple
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
import streamlit as st

class FeatureSearchEngine:
    """
    ë§ˆì¼€íŒ… í”¼ì²˜ ê²€ìƒ‰ ë° ì„ ì •ì´ìœ (Reasoning) ì¶”ì¶œì„ ë‹´ë‹¹í•˜ëŠ” ì—”ì§„ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    
    ì´ í´ë˜ìŠ¤ëŠ” FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ë©°, 
    OpenAIì˜ ì„ë² ë”© ëª¨ë¸ê³¼ LLMì„ í™œìš©í•˜ì—¬ ìµœì ì˜ í”¼ì²˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """
        ì´ˆê¸° ì„¤ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. 
        API í‚¤ í™•ì¸, ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”, í”¼ì²˜ ë°ì´í„° ë¡œë”© ë° ì¸ë±ì‹±ì„ ì§„í–‰í•©ë‹ˆë‹¤.
        """
        # API Key í™•ì¸ (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” Streamlit Secrets)
        api_key = os.getenv("OPENAI_API_KEY")
        try:
            if not api_key and "OPENAI_API_KEY" in st.secrets:
                api_key = st.secrets["OPENAI_API_KEY"]
        except Exception:
            pass

        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small", 
            openai_api_key=api_key
        )
        self.llm = ChatOpenAI(
            model="gpt-4o", 
            temperature=0, 
            openai_api_key=api_key
        )
        self.vector_store = None
        self._initialize_features()

    def _initialize_features(self):
        """
        ê³ í’ˆì§ˆì˜ ê³ ìœ  í”¼ì²˜ ì…‹ì„ ìƒì„±í•˜ê³  FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ì •ëŸ‰ì  ì§€í‘œì™€ í•¨ê»˜ êµ¬ì²´ì ì¸ í–‰ë™ ê·¼ê±°(URL, ê°€ë§¹ì  ë“±)ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.
        """
        # ì¹´í…Œê³ ë¦¬ë³„ ê³ ìœ  í”¼ì²˜ ë° ì‹¤ì œ ì¦ê±° ë°ì´í„° ì •ì˜
        feature_definitions = [
            # Psychographic
            {"cat": "Psychographic (ì‹¬ë¦¬/ë¼ì´í”„ìŠ¤íƒ€ì¼)", "name": "ì–¼ë¦¬ì–´ë‹µí„° ì§€ìˆ˜", "unit": "íšŒ", "time_unit": "ë¶„", "evidence": "GeekNews/Bloter IT ë‰´ìŠ¤ êµ¬ë…"},
            {"cat": "Psychographic (ì‹¬ë¦¬/ë¼ì´í”„ìŠ¤íƒ€ì¼)", "name": "í•´ì™¸ íŠ¸ë Œë“œ ë¯¼ê°ë„", "unit": "íšŒ", "time_unit": "ë¶„", "evidence": "Reddit/Twitch í•´ì™¸ ì»¤ë®¤ë‹ˆí‹° ì ‘ì†"},
            {"cat": "Psychographic (ì‹¬ë¦¬/ë¼ì´í”„ìŠ¤íƒ€ì¼)", "name": "ê°€ì¹˜ ì†Œë¹„ ì„±í–¥", "unit": "ê±´", "time_unit": "ë¶„", "evidence": "ì™€ë””ì¦ˆ/í…€ë¸”ë²… í€ë”© ì°¸ì—¬"},
            {"cat": "Psychographic (ì‹¬ë¦¬/ë¼ì´í”„ìŠ¤íƒ€ì¼)", "name": "ì‚¼ì„± ë¸Œëœë“œ ì„ í˜¸ë„", "unit": "íšŒ", "time_unit": "íšŒ", "evidence": "ì‚¼ì„±ë‹·ì»´/ì‚¼ì„±ë©¤ë²„ìŠ¤ í™œë™ ì´ë ¥"},
            {"cat": "Psychographic (ì‹¬ë¦¬/ë¼ì´í”„ìŠ¤íƒ€ì¼)", "name": "ì• í”Œ ë¸Œëœë“œ ì¶©ì„±ë„", "unit": "íšŒ", "time_unit": "íšŒ", "evidence": "ì• í”ŒìŠ¤í† ì–´/Apple ì „ìš© ì„œë¹„ìŠ¤ ê²°ì œ"},
            
            # Behavioral - Purchase
            {"cat": "Behavioral - Purchase (ì†Œë¹„ í–‰ë™)", "name": "ì‹ì˜ì£¼ ê³ ê´€ì—¬ ì†Œë¹„", "unit": "ê±´", "time_unit": "ë¶„", "evidence": "ë§ˆì¼“ì»¬ë¦¬ ìƒ›ë³„ë°°ì†¡ ë° ë¬´ì‹ ì‚¬ êµ¬ë§¤"},
            {"cat": "Behavioral - Purchase (ì†Œë¹„ í–‰ë™)", "name": "ì»¤í”¼ í•˜ì´ì—”ë“œ ì·¨í–¥", "unit": "íšŒ", "time_unit": "íšŒ", "evidence": "ìŠ¤íƒ€ë²…ìŠ¤ ë¦¬ì €ë¸Œ/ë¸”ë£¨ë³´í‹€ ê²°ì œ"},
            {"cat": "Behavioral - Purchase (ì†Œë¹„ í–‰ë™)", "name": "ë°°ë‹¬ ì„œë¹„ìŠ¤ ì˜ì¡´ë„", "unit": "íšŒ", "time_unit": "ë¶„", "evidence": "ë°°ë‹¬ì˜ë¯¼ì¡±/ì¿ íŒ¡ì´ì¸  ê³ ë¹ˆë„ ì£¼ë¬¸"},
            
            # Behavioral - Digital
            {"cat": "Behavioral - Digital (ë””ì§€í„¸ í–‰ë™)", "name": "ì»¤ë®¤ë‹ˆí‹° í—¤ë¹„ ìœ ì €", "unit": "íšŒ", "time_unit": "ë¶„/ì¼", "evidence": "ì—í¨ì½”ë¦¬ì•„/í´ë¦¬ì•™ ì²´ë¥˜"},
            {"cat": "Behavioral - Digital (ë””ì§€í„¸ í–‰ë™)", "name": "ì¤‘ê³ ê±°ë˜ ì•¡í‹°ë¸Œ ë ˆì´íŒ…", "unit": "ê±´", "time_unit": "íšŒ", "evidence": "ë‹¹ê·¼ë§ˆì¼“ ë§¤ë„ˆì˜¨ë„ ë° ê±°ë˜"},
            {"cat": "Behavioral - Digital (ë””ì§€í„¸ í–‰ë™)", "name": "ìˆí¼ ì½˜í…ì¸  ì†Œë¹„ë ¥", "unit": "íšŒ", "time_unit": "ë¶„/ì¼", "evidence": "í‹±í†¡/ìœ íŠœë¸Œ ì‡¼ì¸  ì‹œì²­"},
            
            # Finance & Risk
            {"cat": "Finance & Risk (ê¸ˆìœµ/ë¦¬ìŠ¤í¬)", "name": "ìì‚° ì„±ìˆ™ë„", "unit": "íšŒ ì ‘ì†", "time_unit": "ë¶„", "evidence": "í† ìŠ¤/ì¹´ì¹´ì˜¤ë±…í¬ ìì‚° ì—°ë™"},
            {"cat": "Finance & Risk (ê¸ˆìœµ/ë¦¬ìŠ¤í¬)", "name": "íˆ¬ì ê³µê²©ì„±", "unit": "íšŒ ê±°ë˜", "time_unit": "ë¶„", "evidence": "í‚¤ì›€ì¦ê¶Œ/ë¯¸ë˜ì—ì…‹ì¦ê¶Œ ë“± ì£¼ìš” ì¦ê¶Œì‚¬ ì‚¬ì´íŠ¸ ì ‘ì†"},
            
            # Customer Journey
            {"cat": "Customer Journey (ê³ ê° ì—¬ì •)", "name": "ì´íƒˆ ì¡°ì§ ê³ ìœ„í—˜êµ°", "unit": "íšŒ", "time_unit": "ì¼", "evidence": "ìµœê·¼ í•œ ë‹¬ê°„ ì•± ë¯¸ì ‘ì†"},
            {"cat": "Customer Journey (ê³ ê° ì—¬ì •)", "name": "ë¸Œëœë“œ ì˜¹í˜¸ì(NPS)", "unit": "íšŒ", "time_unit": "ë¶„", "evidence": "ìë°œì  ìƒí’ˆ í›„ê¸° ì‘ì„±"}
        ]

        documents = []
        segments = ["ì„œìš¸ê¶Œ", "MZì„¸ëŒ€", "ì§ì¥ì¸", "ê³ ì†Œë“ì¸µ", "íŠ¸ë Œë“œì„¸í„°"]
        
        for i, feat in enumerate(feature_definitions):
            for seg in segments:
                feat_name = f"{feat['name']} ({seg})"
                
                # ì •ëŸ‰ì  ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
                count_val = random.randint(10, 150)
                time_val = random.randint(20, 300)
                recency_val = random.randint(1, 14)
                
                # ì¶”ì„¸ ë°ì´í„°
                trend_w = random.choice(["ì¦ê°€", "ìœ ì§€", "ê°ì†Œ"])
                trend_m = random.choice(["ì¦ê°€", "ìœ ì§€", "ê°ì†Œ"])
                
                desc = f"{feat['cat']} ë¶„ì•¼ì˜ {feat['name']} ì§€í‘œì…ë‹ˆë‹¤."
                
                doc = Document(
                    page_content=f"í”¼ì²˜ëª…: {feat_name}, ì¹´í…Œê³ ë¦¬: {feat['cat']}, ì„¤ëª…: {desc}",
                    metadata={
                        "id": i * len(segments) + segments.index(seg) + 1,
                        "name": feat_name,
                        "category": feat['cat'],
                        "evidence": feat['evidence'],
                        "count": f"{count_val}{feat['unit']}",
                        "time": f"{time_val}{feat['time_unit']}",
                        "recency": f"{recency_val}ì¼ ì „",
                        "trend_weekly": trend_w,
                        "trend_monthly": trend_m
                    }
                )
                documents.append(doc)

        self.vector_store = FAISS.from_documents(documents, self.embeddings)

    def search_and_reason(self, plan_data: dict, k: int = 20) -> List[Dict[str, Any]]:
        """
        ì¶”ì¶œëœ ìº í˜ì¸ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬ í”¼ì²˜ë¥¼ ê²€ìƒ‰í•˜ê³  í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ìœ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ 0~1 ì‚¬ì´ë¡œ ì •ê·œí™”í•˜ê³  ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
        """
        query = f"ìƒí’ˆ: {plan_data.get('product', '')}, ë§ˆì¼€íŒ… ì„±ê³µ ì§€í‘œ: {plan_data.get('metric', '')}"
        docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)
        
        results = []
        for doc, score in docs_with_scores:
            meta = doc.metadata
            # FAISS L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„(0~1)ë¡œ ë³€í™˜: 1 / (1 + score) ì‹ ì‚¬ìš©
            # ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡(ê±°ë¦¬ê°€ ì§§ì„ìˆ˜ë¡) 1ì— ìˆ˜ë ´í•¨
            similarity = round(1.0 / (1.0 + score), 4)
            reason = self._generate_reasoning(meta, plan_data)
            
            results.append({
                "ë²ˆí˜¸": meta["id"],
                "í”¼ì²˜ëª…": meta["name"],
                "ì¹´í…Œê³ ë¦¬": meta["category"],
                "ìœ ì‚¬ë„": similarity,
                "ì‚¬ìœ ": reason
            })
            
        # ìœ ì‚¬ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        results.sort(key=lambda x: x["ìœ ì‚¬ë„"], reverse=True)
        return results

    def _generate_reasoning(self, feature_meta: dict, plan_data: dict) -> str:
        """
        ì •ëŸ‰ì  ì§€í‘œì™€ ì£¼ìš” í–‰ë™ ë°œìƒì²˜ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬ìœ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        evidence = feature_meta.get('evidence', 'ê¸°ë³¸ í™œë™ ì´ë ¥')
        count = feature_meta.get('count', '-')
        time = feature_meta.get('time', '-')
        recency = feature_meta.get('recency', '-')
        t_w = feature_meta.get('trend_weekly', '-')
        t_m = feature_meta.get('trend_monthly', '-')
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì‚¬ìœ : í–‰ë™ ë°œìƒì²˜ + ì •ëŸ‰ ì§€í‘œ + ì¶”ì„¸
        reason = (
            f"ğŸ“ **ì£¼ìš” í–‰ë™ ë°œìƒì²˜**: `{evidence}`  \n"
            f"ğŸ“Š **ì •ëŸ‰ ì§€í‘œ**: ë°œìƒ {count} / ì´ìš©ì‹œê°„ {time} / **ìµœê·¼ {recency} ë°œìƒ**  \n"
            f"ğŸ“ˆ **ì¶”ì„¸ ë¶„ì„**: ìµœê·¼ 1ì£¼ì¼ {t_w} / ìµœê·¼ 1ë‹¬ {t_m} ì¶”ì„¸"
        )
        return reason
